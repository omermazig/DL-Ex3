We conducted an experiment using the California Housing dataset, employing networks with 2/3 depth as instructed. The models consisted of 5 hidden layers and did not include bias, which allowed for a comparison with the e2e dynamics that also exclude bias.

The results, depicted in the accompanying graph, revealed that both models (2/3 layers) exhibited similar loss values after 50,000 epochs. Additionally, the "loss" obtained by calculating the e2e dynamics matrix was also comparable for both 2 and 3 layers.

This outcome can be attributed to the dataset itself, which does not pose significant challenges for classification. Hence, the additional layer does not contribute significantly, and both 2 and 3-layer models perform similarly.

It is noteworthy that the loss achieved by the e2e-dynamic calculation closely approximated the loss obtained by the actual model! Ideally, these losses should be exactly the same, and in a previous accidental scenario where I used inputs of size 1 (instead of 8, as given by the California Housing dataset), the losses did match. However, we attribute this discrepancy to factors such as a too large learning rate, insufficient epochs, or numerical instability.